{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding vectors: see paper [here](http://dl.acm.org/citation.cfm?id=2999959)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For protein sequences, I have seen that using hashing for k-mers works well, however, it would be interesting to use something similar to embeddings, a way for encoding words depending on frequency. \n",
    "\n",
    "I am not sure if keras can do this, but tensorflow provides a good example. In any case this is just an input encoding, basically independent of the learning process, It could be done using tensorflow if keras is not helpful on this. The basic principle is to maximize the average log probability within some context:\n",
    "\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p( \\omega_{t+j} | \\omega_t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
